# Reinforcement Learning이란...?

## 개념

강화학습 : 
  - 머신러닝의 한 종류
  - 어떠한 환경에서 어떠한 행동을 했을 때, 그 행동이 잘 된 행동인지 아닌지를 나중에 판단하고 보상(혹은 벌칙)을 줌으로써, 스스로 학습하게 하는 분야
 
순서 : 
  - Agent : 특정 환경에서 행동을 결정
  - Envir : 그 결정에 대한 보상을 내림 ( 한 번에 결정보단, 여러 행동에 대한 추후 판단이 많음)

## 마르코프 의사 결정 과정 (Markov Decision Process, MDP)
### 마르코프 가정

"상태가 연속적인 시간에 따라 이어질 때 어떠한 시점의 상태는 그 시점 바로 이전의 상태에만 영향을 받는다는 가정"
- 이전 상태의 결과는 그 이전 상태를 반영하고 있다.
- 따라서 현재 상태는 그 현재 이전의 모든 상태를 누적하고 있는 직전 상태만 사용해도 된다.

### 마르코프 과정
마르코프 과정은 마르포크 가정을 만족하는 연속적인 일련의 상태
- 일련의 상태 <S1, S2, ... St>와 상태 전이 확률로 구성
- 상태전이확률은 어떠한 상태가 i일 때, 그 다음 상태가 j가 될 확률을 의미
  - P(A|B) : B상태일 때, A가 등장할 확률의 개념


### 마르코프 의사 결정과정
MDP의 구성
- S : 상태(s,state) 집합
- A : 행동(a,action) 집합
- P : 상태 전이 확률(state transition probability) 행렬
- R : 보상(r,reward) 집합
- g : 할인 요인(discount factor)

상황별 설명
- 다음 행동 확률
  - P[S_t+1 = s' | S_t = s, A_t = a]
  - 상태가 s이고, action이 a일 때, 다음 행동이 s'으로 변할 확률
- Reward
  - E[R_t+1 | S = s, A_t = a]
  - 상태가 s이고, action이 a일 때, t+1번째의 보상의 기댓값
- 할인 요인 (과거의 행동을 얼마나 반영할지에 대한 지표)
  - 할인 요인이 1이면 <1,1,1,1,1>
  - 할인 요인이 0.9면 <1, 0.9, 0.81, 0.729, 0.6561>
- 정책
  - 어떠한 상태 s에서 수행할 행동 a를 정하는 방법
  - 특정 상태에 따른 action을 다르게 수행한다.

## 상태 가치 함수 & 상태-행동 가치 함수
### 상태 가치 함수
Agent가 어떠한 행동을 수행하면서 상태가 시간에 따라 변화
  - 특정 상태 s에서 얻은 보상은, 현재 상태가 s일 때, 모든 보상에 할인율을 곱한 것의 기대값

### 상태-행동 가치 함수
어떤 상태 s에서 행동 a를 수행했을 때의 가치(Q-Value)

## 벨만 방정식
### 벨만 기대 방정식
상태 가치 함수의 벨만 기대 방정식
- 현재 상태 St에서의 가치는 다음 상태의 가치에 할인율을 곱해 더한 기댓값

상태-행동 가치 함수의 벨만 기대 방정식
- 현재 상태 St와 행동 At에 따른 가치에 할인율을 곱해 보상에 더한 값


상태 가치 함수
- (상태 s에서 정책 pi에 의해 행동 a를 선택할 확률) * (상태 s에서 정책 pi에 의해 행동 a를 수행했을 때의 가치)
- (상태 s에서 정책 pi에 의해 행동 a를 선택할 확률) * (상태 s에서 행동 a를 수행했을 때 기대 보상 + 상태 s'에 대한 기대 가치 * 할인율)

상태-행동 가치 함수를 재귀적으로 표현 가능

### 벨만 최적 방정식
가장 큰 총 보상을 받을 수 있는 정책을 따랐을 때 얻을 가치
- 상태 가치함수 V를 최대화
- 상태-행동 가치 함수 Q를 최대화
